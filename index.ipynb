{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Coefficient of Determination - Lab\n", "\n", "## Introduction\n", "In the previous lesson, you looked at the Coefficient of Determination, what it means, and how it is calculated. In this lesson, you'll use the R-Squared formula to calculate it in Python and NumPy. \n", "\n", "## Objectives\n", "\n", "You will be able to:\n", "\n", "* Calculate the coefficient of determination using self-constructed functions\n", "* Use the coefficient of determination to determine model performance\n", "\n", "\n", "## Let's get started\n", "\n", "Once a regression model is created, we need to decide how \"accurate\" the regression line is to some degree. \n", "\n", "\n", "Here is the equation for R-Squared or the Coefficient of Determination again: \n", "\n", "$$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1- \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $$\n", " \n", " Note that this is also equal to:\n", "\n", "$$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}}=\\dfrac{SS_{EXP}}{SS_{TOT}} $$\n", "where\n", "\n", "- $SS_{TOT} = \\sum_i(y_i - \\overline y_i)^2$ $\\rightarrow$ Total Sum of Squares  \n", "- $SS_{EXP} = \\sum_i(\\hat y_i - \\overline y_i)^2$ $\\rightarrow$  Explained Sum of Squares\n", "- $SS_{RES}= \\sum_i(y_i - \\hat y_i)^2 $ $\\rightarrow$ Residual Sum of Squares\n", "\n", "Recall that the objective of $R^2$ is to learn how much of the error is a result in variation in the data features, as opposed to being a result of the regression line being a poor fit."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Programming R-Squared\n", "\n", "Let's calculate R-Squared in Python. We'll use these y variables:"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "Y = np.array([1, 3, 5, 7])\n", "Y_pred = np.array([4.1466666666666665, 2.386666666666667, 3.56, 5.906666666666666])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["* `Y` represents the actual values, i.e. $y$\n", "* `Y_pred` represents the model's predictions, i.e. $\\hat{y}$\n", "\n", "Note that we do not actually need to have a regression equation or x values to calculate R-Squared!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We'll break down the problem of calculating R-Squared into two steps:\n", "\n", "1. A function `sq_error` that calculates the sum of squared error between any two arrays of y values\n", "2. A function `r_squared` that uses `sq_error` to calculate the R-Squared value"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Calculating Squared Error\n", "\n", "The first step is to calculate the sum of squared error. Remember that the sum of squared error is the sum of the squared differences between two sets of values.\n", "\n", "Create a function `sq_err()` that takes in y points for 2 arrays, calculates the difference between corresponding elements of these arrays, squares the differences, and sums all the squared differences."]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"data": {"text/plain": ["13.546666666666667"]}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": ["# Calculate sum of squared errors between two sets of y values\n", "\n", "def sq_err(y_1, y_2):\n", "    differences = (y_1 - y_2)\n", "    differences_squared = differences ** 2\n", "    sum_of_squared_differences = differences_squared.sum()\n", "    return sum_of_squared_differences\n", "\n", "sq_err(Y, Y_pred) # should return about 13.55"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Calculating R-Squared\n", "\n", "Squared error, as calculated above is only a part of the coefficient of determination. Let's now build a function that uses the `sq_err` function above to calculate the value of R-Squared.\n", "\n", "Remember, R-Squared is the explained sum of squares divided by the total sum of squares.\n", "\n", "* Create a variable `y_mean` that represents the mean for each value of y\n", "* Calculate ESS (i.e. $SS_{EXP}$) by passing `y_predicted` and `y_mean` into the `sq_err` function\n", "* Calculate TSS (i.e. $SS_{TOT}$) by passing `y_real` and `y_mean` into the `sq_err` function\n", "* Calculate R-Squared by dividing ESS by TSS"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.32266666666666655"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["\n", "def r_squared(y_real, y_predicted):\n", "    # calculate the mean\n", "    y_mean = np.array([y_real.mean() for y in y_real])\n", "    \n", "    # calculate the numerator\n", "    ess = sq_err(y_predicted, y_mean)\n", "    # calculate the denominator\n", "    tss = sq_err(y_real, y_mean)\n", "    \n", "    return ess/tss\n", "\n", "r_squared(Y, Y_pred) # should return about 0.32"]}, {"cell_type": "markdown", "metadata": {}, "source": ["What does this R-Squared mean?\n", "\n", "---\n", "\n", "<details>\n", "    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\n", "\n", "The model that produced `Y_pred` is explaining about 32.3% of the variance in `Y`. It depends on what these values represent, whether this R-Squared is good enough for our use case.\n", "\n", "</details>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary\n", "In this lesson, you learned how to calculate R-Squared using Python and NumPy. You also interpreted the result in terms of explained variance. Later on you'll learn how to use StatsModels to compute R-Squared for you!"]}], "metadata": {"kernelspec": {"display_name": "Python (learn-env)", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 2}